import os
import json
import re
import time
from googleapiclient.discovery import build
from google.oauth2 import service_account
from datetime import datetime, timedelta, timezone
from googleapiclient.errors import HttpError
import html

# å¾ç’°å¢ƒè®Šé‡ä¸­è®€å– Google API æ†‘è­‰
google_sheets_credentials = os.getenv('GOOGLE_SHEETS_CREDENTIALS')
google_api_key = os.getenv('GOOGLE_API_KEY')

if not google_sheets_credentials or not google_api_key:
    raise ValueError("ç¼ºå°‘Google APIæ†‘è­‰æˆ–APIå¯†é‘°")

try:
    credentials_info = json.loads(google_sheets_credentials)
    credentials = service_account.Credentials.from_service_account_info(credentials_info)
except Exception as e:
    raise ValueError("ç„¡æ•ˆçš„Google Sheetsæ†‘è­‰") from e

# YouTube Data API å®¢æˆ¶ç«¯
youtube = build('youtube', 'v3', developerKey=google_api_key)

def get_video_date(video_id):
    """ç²å–å½±ç‰‡çš„å¯¦éš›ç›´æ’­æ—¥æœŸ"""
    try:
        request = youtube.videos().list(
            part='liveStreamingDetails,snippet',
            id=video_id
        )
        response = request.execute()
        
        if not response['items']:
            print(f"DEBUG: ç„¡æ³•æ‰¾åˆ°å½±ç‰‡ {video_id} çš„è³‡è¨Š")
            return None
        
        video_details = response['items'][0]
        
        # æ·»åŠ èª¿è©¦è¨Šæ¯
        print(f"DEBUG: å½±ç‰‡ {video_id} çš„è³‡è¨Šï¼š")
        print(f"DEBUG: å½±ç‰‡é¡å‹: {video_details['snippet'].get('liveBroadcastContent')}")
        print(f"DEBUG: æ¨™é¡Œ: {video_details['snippet'].get('title')}")
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºæœƒå“¡é™å®šç›´æ’­
        if video_details['snippet'].get('liveBroadcastContent') == 'membersOnly':
            print(f"DEBUG: è·³éæœƒå“¡é™å®šè¦–é »ï¼š{video_id}")
            return None
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºç›´æ’­å½±ç‰‡
        if 'liveStreamingDetails' in video_details:
            # ä½¿ç”¨ç›´æ’­é–‹å§‹æ™‚é–“
            actual_start_time = video_details['liveStreamingDetails'].get('actualStartTime')
            if actual_start_time:
                # ç›´æ¥è§£ææ™‚é–“ï¼Œé€™å·²ç¶“æ˜¯ç•¶åœ°æ™‚é–“
                stream_date = datetime.strptime(actual_start_time, '%Y-%m-%dT%H:%M:%SZ')
                return stream_date.date()
        
        # å¦‚æœä¸æ˜¯ç›´æ’­æˆ–æ²’æœ‰ç›´æ’­æ™‚é–“ï¼Œä½¿ç”¨ç™¼å¸ƒæ™‚é–“
        publish_time = video_details['snippet']['publishedAt']
        publish_date = datetime.strptime(publish_time, '%Y-%m-%dT%H:%M:%SZ')
        return publish_date.date()
    except HttpError as e:
        print(f"Error fetching video date for {video_id}: {e}")
        return None

def get_video_ids_from_playlist(playlist_id):
    """å¾æ’­æ”¾æ¸…å–®ç²å–å½±ç‰‡IDå’Œæ—¥æœŸ"""
    video_info = []
    
    # è¨ˆç®—æœ€è¿‘30å¤©çš„æ—¥æœŸ
    thirty_days_ago = datetime.now(timezone.utc) - timedelta(days=30)
    # æ ¼å¼åŒ–æˆ RFC 3339 æ ¼å¼
    published_after = thirty_days_ago.strftime('%Y-%m-%dT%H:%M:%SZ')
    
    request = youtube.playlistItems().list(
        part='contentDetails,snippet',  # æ·»åŠ  snippet
        playlistId=playlist_id,
        maxResults=50
    )
    
    while request:
        try:
            response = request.execute()
            for item in response['items']:
                video_id = item['contentDetails']['videoId']
                video_date = get_video_date(video_id)
                
                if video_date and video_date >= thirty_days_ago.date():
                    video_info.append((video_id, video_date))
                    print(f"æ‰¾åˆ°å½±ç‰‡ï¼š{video_id} ä¾†è‡ª {video_date}")
                    
            request = youtube.playlistItems().list_next(request, response)
        except HttpError as e:
            print(f"Error fetching playlist items: {e}")
            break
    
    return video_info

def get_video_ids_from_channel(channel_id):
    """å¾é »é“ç²å–å½±ç‰‡IDå’Œæ—¥æœŸ"""
    video_info = []
    
    # è¨ˆç®—æ™‚é–“ç¯„åœ
    current_time = datetime.now(timezone.utc)
    thirty_days_ago = current_time - timedelta(days=30)
    
    # æ ¼å¼åŒ–æ™‚é–“ç‚º ISO 8601 æ ¼å¼
    published_after = thirty_days_ago.strftime('%Y-%m-%dT%H:%M:%SZ')
    published_before = current_time.strftime('%Y-%m-%dT%H:%M:%SZ')
    
    print(f"DEBUG: æœå°‹æ™‚é–“ç¯„åœ: {published_after} åˆ° {published_before}")
    
    request = youtube.search().list(
        part='snippet',
        channelId=channel_id,
        maxResults=50,
        order='date',
        type='video',
        publishedAfter=published_after,
        publishedBefore=published_before
    )
    
    while request:
        try:
            response = request.execute()
            items = response.get('items', [])
            print(f"DEBUG: ç²å–åˆ° {len(items)} å€‹å½±ç‰‡")
            
            for item in response['items']:
                video_id = item['id']['videoId']
                snippet = item['snippet']
                title = snippet['title'].lower()
                print(f"DEBUG: æª¢æŸ¥å½±ç‰‡: {snippet['title']}")
                
                # æª¢æŸ¥æ¨™é¡Œæ˜¯å¦åŒ…å«ä»»ä½•ç›¸é—œé—œéµå­—
                keywords = ['æ­Œæ ', 'karaoke', 'ã‚«ãƒ©ã‚ªã‚±', 'singing']
                if any(keyword.lower() in title for keyword in keywords):
                    video_date = get_video_date(video_id)
                    if video_date:
                        video_info.append((video_id, video_date))
                        print(f"DEBUG: æ‰¾åˆ°ç¬¦åˆçš„å½±ç‰‡ï¼š{video_id} - {snippet['title']} - {video_date}")
                else:
                    print(f"DEBUG: ä¸ç¬¦åˆé—œéµå­—çš„å½±ç‰‡ï¼š{snippet['title']}")
                
            # æª¢æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é 
            if 'nextPageToken' in response:
                request = youtube.search().list_next(request, response)
            else:
                request = None
                
        except HttpError as e:
            print(f"DEBUG: API éŒ¯èª¤: {str(e)}")
            break
            
    print(f"DEBUG: ç¸½å…±æ‰¾åˆ° {len(video_info)} å€‹ç¬¦åˆçš„å½±ç‰‡")
    return video_info
    
def get_timestamp_comment(video_id):
    """ç²å–åŒ…å«æ™‚é–“æˆ³æ¨™è¨˜çš„ç•™è¨€"""
    try:
        request = youtube.commentThreads().list(
            part='snippet,replies',
            videoId=video_id,
            maxResults=100
        )

        while request:
            response = request.execute()
            for item in response['items']:
                # æª¢æŸ¥é ‚ç´šè©•è«–
                comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
                if 'ğŸ’ğŸŒŸğŸ¶ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ğŸ’ğŸŒŸğŸ¶' in comment or 'ğŸŒŸğŸ’ğŸ¶ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ğŸŒŸğŸ’ğŸ¶' in comment:
                    return comment
                
                # æª¢æŸ¥å›è¦†è©•è«–
                if 'replies' in item:
                    for reply in item['replies']['comments']:
                        reply_comment = reply['snippet']['textDisplay']
                        if 'ğŸ’ğŸŒŸğŸ¶ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ğŸ’ğŸŒŸğŸ¶' in reply_comment or 'ğŸŒŸğŸ’ğŸ¶ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ğŸŒŸğŸ’ğŸ¶' in reply_comment:
                            return reply_comment

            request = youtube.commentThreads().list_next(request, response)
    
    except HttpError as e:
        print(f"Error fetching comments for video {video_id}: {e}")
        return None

    return None

def clean_html(raw_html):
    """ç§»é™¤HTMLæ¨™ç±¤ä¸¦è™•ç†æ›è¡Œå’Œç‰¹æ®Šå­—ç¬¦"""
    clean_text = re.sub(r'<br\s*/?>', '\n', raw_html)  # æ›¿æ› <br> ç‚ºæ›è¡Œç¬¦
    clean_text = re.sub(r'<.*?>', '', clean_text)  # ç§»é™¤å…¶ä»–HTMLæ¨™ç±¤
    clean_text = html.unescape(clean_text)  # è½‰æ›HTMLå¯¦é«”ç‚ºæ™®é€šå­—ç¬¦
    return clean_text

def save_to_file(video_id, comment, date):
    """ä¿å­˜ç•™è¨€åˆ°æ–‡ä»¶"""
    if not comment:
        print(f"æœªæ‰¾åˆ°æ™‚é–“æˆ³ç•™è¨€ï¼Œè¦–é »IDï¼š{video_id}")
        return
        
    output_dir = 'timeline'
    os.makedirs(output_dir, exist_ok=True)
    
    file_name = date.strftime('%Y%m%d') + '.txt'
    file_path = os.path.join(output_dir, file_name)
    
    # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    if os.path.exists(file_path):
        print(f"æ—¥æœŸç‚º {file_name} çš„æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³éã€‚")
        return
    
    # æ¸…ç†HTMLæ¨™ç±¤
    clean_comment = clean_html(comment)
    
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(f'ID = {video_id}\n')
        f.write(clean_comment)
        
    print(f"å·²ä¿å­˜æ™‚é–“æˆ³ç•™è¨€åˆ° {file_path}")

def main():
    playlist_id = 'PL7H5HbMMfm_lUoLIkPAZkhF_W0oDf5WEk'
    channel_id = 'UCwBJ-8LQYd7lZ7uW-4Adt4Q'

    print(f"DEBUG: é–‹å§‹æœå°‹æ™‚é–“: {datetime.now(timezone.utc)}")
    
    # å¾é »é“ç²å–æ‰€æœ‰å½±ç‰‡
    video_info = get_video_ids_from_channel(channel_id)
    print(f"DEBUG: å¾é »é“ç²å–åˆ° {len(video_info)} å€‹å½±ç‰‡")
    
    # å¾æ’­æ”¾æ¸…å–®ç²å–
    playlist_videos = get_video_ids_from_playlist(playlist_id)
    print(f"DEBUG: å¾æ’­æ”¾æ¸…å–®ç²å–åˆ° {len(playlist_videos)} å€‹å½±ç‰‡")
    
    video_info.extend(playlist_videos)
    
    # ç§»é™¤é‡è¤‡å‰çš„æ•¸é‡
    print(f"DEBUG: å»é‡å‰ç¸½æ•¸: {len(video_info)}")
    
    # ç§»é™¤é‡è¤‡
    video_info = list(set(video_info))
    print(f"DEBUG: å»é‡å¾Œç¸½æ•¸: {len(video_info)}")
    
    batch_size = 10
    for i in range(0, len(video_info), batch_size):
        batch_videos = video_info[i:i + batch_size]
        for video_id, video_date in batch_videos:
            file_name = video_date.strftime('%Y%m%d') + '.txt'
            file_path = os.path.join('timeline', file_name)
            if os.path.exists(file_path):
                print(f"æ—¥æœŸç‚º {file_name} çš„æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³éè¦–é »ID {video_id}ã€‚")
                continue

            print(f"è™•ç†è¦–é » {video_id} ä¾†è‡ª {video_date}")
            timestamp_comment = get_timestamp_comment(video_id)
            if timestamp_comment:
                save_to_file(video_id, timestamp_comment, video_date)
        time.sleep(2)

if __name__ == '__main__':
    main()
